# 1.1 Train a model without attention
Epoch 1: loss=0.047994825091612604, BLEU=0.22733198802530794
Epoch 2: loss=0.043513103674606424, BLEU=0.2594208233555698
Epoch 3: loss=0.04138702632541533, BLEU=0.27265775040075424
Epoch 4: loss=0.03975627649594246, BLEU=0.28310189623071175
Epoch 5: loss=0.0384875027681959, BLEU=0.2880838590329372
Finished 5 epochs

# 1.2 Train a model with attention
Epoch 1: loss=0.046506362589595125, BLEU=0.27429651446303893
Epoch 2: loss=0.041660766332075695, BLEU=0.3047057228103407
Epoch 3: loss=0.039473071378113296, BLEU=0.3147467976377459
Epoch 4: loss=0.037940053158822554, BLEU=0.3221965618735416
Epoch 5: loss=0.03681648656564964, BLEU=0.32342431156503704
Finished 5 epochs

# 1.3 Train a model with multi-head attention

# 2.1 Test the model without attention
The average BLEU score over the test set was 0.32287724817917884

# 2.2 Test the model with attention
The average BLEU score over the test set was 0.3650570831641825

# 2.3 Test the model with multi-head attention

# 3 A brief discussion on your findings:
3.1 Was there a discrepancy in between training and testing results?
The average BLEU scores over the test sets are higher than train sets, which is not what I expected since normally the
accuracy in training part is better than testing part. This situation happened might because:
a. It might because of the evaluating/testing method. In this assignment, it used hold-out testing method
(make sense, since it has a large dataset). But compared to the cross-validation method, Hold-out is dependent on just one train-test split,
which makes the hold-out method score dependent on how the data is split into train and test sets.
We could use Cross-validation method(i.e. k-fold cross validation), in which the dataset is randomly split up into ‘k’ groups,
over different cell types or other parameter.
b. It might because the test set is simpler than train set, in other word, the testing is easier than training.
Even though this shouldn't happen since we sampled the data(with large size) by the natural underlying distribution.
But it's possible the test set be sampled less bias and more prototypical, which will cause higher accuracy.
c. It might because the models generated well. It shouldn't be over-fitting to train/test set,
since the BLEU scores are much less than 1.


3.2 If one model did better than the others, why do you think that is?
The model with attention performed better than the model without attention.
(I only compared 2 models since I didn't finish multi-head model)
This situation happened might because:
a. With-attention model has a better prior choice. Compared to without-attention model, with-attention model doesn't
depend equally on all words prior, then it will use a better posterior estimate of which words to consider at the current timestep.
b. Instead of encoding the input sequence into a single fixed context vector, the attention model develops a context vector
that is filtered specifically for each output time step.
c. Allow decoder to “attend” to certain areas of input when making decisions

Also, in theory, multi-head model should perform better than single-head model.
Since multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.
With a single attention head, averaging inhibits this.


